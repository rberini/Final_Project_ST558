[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Diabetes Modeling",
    "section": "",
    "text": "In this phase, we turn our attention to the process of fitting and selecting a best model to predict a diabetes diagnosis using the subset of variables described during our Exploratory Data Analysis (EDA) phase.\nAs outlined in the corresponding EDA report, this project considers the Diabetes Health Indicators data set. The data is drawn from a 2015 Behavioral Risk Factor Surveillance System (BRFSS) survey conducted by the Centers for Disease Control and Prevention (CDC).\nThere are 253,680 observations in the data set and 21 potential predictors.\nThis investigation and analysis considers whether answers to the BRFSS survey questions might prove useful in predicting a diabetes diagnosis, and, if so, which subset of potential predictors should be considered in modeling. The goal for this modeling phase is to create models for predicting the Diabetes variable. We use the tidymodels package to consider two models: a classification tree and a random forest. We will use log loss as our metric to evaluate the models. For both model types, we use log loss with 5 fold cross-validation to select the best model from that family of models. We then compare the best model from each family to select one best final model to be published via API."
  },
  {
    "objectID": "Modeling.html#introduction-and-overview",
    "href": "Modeling.html#introduction-and-overview",
    "title": "Diabetes Modeling",
    "section": "",
    "text": "In this phase, we turn our attention to the process of fitting and selecting a best model to predict a diabetes diagnosis using the subset of variables described during our Exploratory Data Analysis (EDA) phase.\nAs outlined in the corresponding EDA report, this project considers the Diabetes Health Indicators data set. The data is drawn from a 2015 Behavioral Risk Factor Surveillance System (BRFSS) survey conducted by the Centers for Disease Control and Prevention (CDC).\nThere are 253,680 observations in the data set and 21 potential predictors.\nThis investigation and analysis considers whether answers to the BRFSS survey questions might prove useful in predicting a diabetes diagnosis, and, if so, which subset of potential predictors should be considered in modeling. The goal for this modeling phase is to create models for predicting the Diabetes variable. We use the tidymodels package to consider two models: a classification tree and a random forest. We will use log loss as our metric to evaluate the models. For both model types, we use log loss with 5 fold cross-validation to select the best model from that family of models. We then compare the best model from each family to select one best final model to be published via API."
  },
  {
    "objectID": "Modeling.html#load-required-packages",
    "href": "Modeling.html#load-required-packages",
    "title": "Diabetes Modeling",
    "section": "Load required packages",
    "text": "Load required packages\n\nlibrary(conflicted)\nlibrary(tidyverse)\nlibrary(ranger) #not directly called; used by tidymodels package\nlibrary(tidymodels)\nlibrary(baguette) #not used; loading in case we later might consider a bagged tree\nlibrary(vip)\nlibrary(future) #attempting to improve tuning time through parallelization\nlibrary(furrr) #not directly called; used by future package\n\nconflicts_prefer(dplyr::lag)\nconflicts_prefer(dplyr::filter)\ntidymodels_prefer()\n\noptions(scipen = 999, digits = 2)"
  },
  {
    "objectID": "Modeling.html#read-data",
    "href": "Modeling.html#read-data",
    "title": "Diabetes Modeling",
    "section": "Read data",
    "text": "Read data\nLoad manipulated data set coming out of EDA phase.\n\ndiabetes &lt;- readRDS(\"data/diabetes.rds\")\ndiabetes\n\n# A tibble: 253,680 × 22\n   Diabetes HighBP HighChol CholCheck   BMI Smoker Stroke HeartDiseaseorAttack\n   &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;               \n 1 No       Yes    Yes      Yes          40 Yes    No     No                  \n 2 No       No     No       No           25 Yes    No     No                  \n 3 No       Yes    Yes      Yes          28 No     No     No                  \n 4 No       Yes    No       Yes          27 No     No     No                  \n 5 No       Yes    Yes      Yes          24 No     No     No                  \n 6 No       Yes    Yes      Yes          25 Yes    No     No                  \n 7 No       Yes    No       Yes          30 Yes    No     No                  \n 8 No       Yes    Yes      Yes          25 Yes    No     No                  \n 9 Yes      Yes    Yes      Yes          30 Yes    No     Yes                 \n10 No       No     No       Yes          24 No     No     No                  \n# ℹ 253,670 more rows\n# ℹ 14 more variables: PhysActivity &lt;fct&gt;, Fruits &lt;fct&gt;, Veggies &lt;fct&gt;,\n#   HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;, NoDocbcCost &lt;fct&gt;,\n#   GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;, DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;,\n#   Age &lt;ord&gt;, Education &lt;ord&gt;, Income &lt;ord&gt;"
  },
  {
    "objectID": "Modeling.html#split-the-data",
    "href": "Modeling.html#split-the-data",
    "title": "Diabetes Modeling",
    "section": "Split the data",
    "text": "Split the data\nSplit the data into training and test sets (70/30 split). Use the strata argument to stratify the split on the diabetes outcome given the low proportion of positive cases in the data set.\n\nset.seed(558)\ndiabetes_split &lt;- initial_split(diabetes, prop = 0.7, strata = Diabetes)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\n\nFor the training set, create a five fold cross-validation (CV) split. Again, we use the strata argument to stratify the folds on the diabetes outcome to appropriately reflect the training-test split. The five fold cross-validation method randomly splits the data set into five equal parts. Four of the splits are used to train the model and the fifth, a holdout or validation set, is used to evaluate performance based on the chosen metric (log loss in this case). Which specific fold is used for the validation set rotates among the five folds at each run. This method can be used to assess performance of the model as applied to data not used to train that model (i.e., the validation fold). In addition, when used with the tuning grid, this method can be used to estimate the best parameters.\n\nset.seed(558)\ndiabetes_5_fold &lt;- vfold_cv(diabetes_train, v = 5, strata = Diabetes)"
  },
  {
    "objectID": "Modeling.html#create-recipe",
    "href": "Modeling.html#create-recipe",
    "title": "Diabetes Modeling",
    "section": "Create recipe",
    "text": "Create recipe\nConstruct a recipe that defines the preprocessing steps for the models to follow. The recipe specifies the response variable (Diabetes) and which predictors to keep and remove (as outlined in the EDA phase). We opt not to normalize numeric predictors because decision trees are not affected by feature scaling.\n\ndiabetes_rec &lt;-\n  recipe(Diabetes ~ ., data = diabetes_train) |&gt;\n  step_rm(CholCheck, Smoker, Stroke, PhysActivity:NoDocbcCost, DiffWalk, Education) #|&gt;\n  #step_normalize(all_numeric_predictors()) this step removed as not needed for trees\n\nAfter creating the recipe, inspect the transformed training data.\n\ndiabetes_rec |&gt;\n  prep(training = diabetes_train) |&gt;\n  bake(diabetes_train)\n\n# A tibble: 177,575 × 11\n   HighBP HighChol   BMI HeartDiseaseorAttack GenHlth MentHlth PhysHlth Sex   \n   &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; \n 1 No     No          25 No                         3        0        0 Female\n 2 Yes    Yes         28 No                         5       30       30 Female\n 3 Yes    No          27 No                         2        0        0 Female\n 4 Yes    No          30 No                         3        0       14 Female\n 5 Yes    Yes         25 No                         3        0        0 Female\n 6 No     No          24 No                         2        0        0 Male  \n 7 No     Yes         33 No                         4       30       28 Female\n 8 Yes    No          33 No                         2        5        0 Female\n 9 Yes    Yes         21 No                         3        0        0 Female\n10 Yes    Yes         38 No                         5       15       30 Female\n# ℹ 177,565 more rows\n# ℹ 3 more variables: Age &lt;ord&gt;, Income &lt;ord&gt;, Diabetes &lt;fct&gt;\n\n\nDefine the metrics set. Our primary metric for training and evaluation is log loss; however, we may wish to consider other measures of performance. So here we also consider accuracy, specificity (i.e., true positive rate), sensitivity (i.e., true negative rate), and receiver operating characteristic area under the curve (representing how well the model can distinguish between positive and negative cases).\n\nmetrics_set &lt;- metric_set(mn_log_loss, roc_auc, accuracy, spec, sens)"
  },
  {
    "objectID": "Modeling.html#fit-classification-tree-model",
    "href": "Modeling.html#fit-classification-tree-model",
    "title": "Diabetes Modeling",
    "section": "Fit Classification Tree model",
    "text": "Fit Classification Tree model\nFirst, consider the Classification Tree family of models. Classification Trees are a type of decision tree that segments data into distinct categories using a hierarchical structure. Beginning with a root node, these trees recursively divide observations based on predictor variables, creating increasingly focused subgroups. At each decision node, the algorithm selects the most informative split that best separates observations into more homogeneous groups. The process continues until reaching leaf nodes, which represent final classification outcomes for similar observations.Classification trees, as opposed to Regression Trees, are employed for categorical response variables. They offer clear, visual representations of decisions, making it easy to understand and explain the classification process.\nSet up the Classification Tree fit to use the “rpart” engine.\n\ntree_mod &lt;- decision_tree(tree_depth = 8,\n                          min_n = 25,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nCreate a workflow using the recipe and model defined earlier.\n\ndiabetes_tree_wfl &lt;- \n  workflow() |&gt;\n  add_recipe(diabetes_rec) |&gt;\n  add_model(tree_mod)\n\nCreate a tuning grid to consider varying levels of cost complexity.\n\nplan(multisession, workers = 4)\n\nset.seed(558)\n\ntree_fits &lt;-\n  diabetes_tree_wfl |&gt; \n  tune_grid(resamples = diabetes_5_fold, grid = 20,\n            metrics = metric_set(mn_log_loss))\n\nplan(sequential)\n\nSort models by the tuned cost complexity parameter associated with lowest log loss.\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 20 × 7\n   cost_complexity .metric     .estimator  mean     n    std_err .config        \n             &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;          \n 1        2.95e- 7 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 2        8.91e- 8 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 3        5.29e- 6 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 4        3.70e- 8 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 5        5.70e- 7 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 6        4.43e- 9 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 7        1.11e- 9 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 8        1.16e- 8 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n 9        4.31e-10 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n10        2.08e- 6 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n11        2.28e-10 mn_log_loss binary     0.332     5 0.00149    Preprocessor1_…\n12        1.99e- 5 mn_log_loss binary     0.333     5 0.00155    Preprocessor1_…\n13        6.48e- 5 mn_log_loss binary     0.335     5 0.000671   Preprocessor1_…\n14        1.90e- 4 mn_log_loss binary     0.350     5 0.00164    Preprocessor1_…\n15        3.81e- 4 mn_log_loss binary     0.355     5 0.00109    Preprocessor1_…\n16        6.66e- 4 mn_log_loss binary     0.355     5 0.000982   Preprocessor1_…\n17        3.63e- 3 mn_log_loss binary     0.355     5 0.000963   Preprocessor1_…\n18        5.86e- 3 mn_log_loss binary     0.374     5 0.0120     Preprocessor1_…\n19        8.90e- 2 mn_log_loss binary     0.404     5 0.00000973 Preprocessor1_…\n20        2.84e- 2 mn_log_loss binary     0.404     5 0.00000973 Preprocessor1_…\n\n\nIdentify the parameter specification with lowest log loss to inform the best model.\n\ntree_best_params &lt;-\n  tree_fits |&gt;\n  select_best(metric = \"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1     0.000000295 Preprocessor1_Model03\n\n\nUsing the best model, fit the model to the entire training data set using the last_fit() function. Compute the log loss and other performance metrics on the test set.\n\ndiabetes_tree_final_fit &lt;-\n  diabetes_tree_wfl |&gt;\n  finalize_workflow(tree_best_params) |&gt;\n  last_fit(split = diabetes_split, metrics = metrics_set)\n\ndiabetes_tree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 5 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.864 Preprocessor1_Model1\n2 spec        binary         0.982 Preprocessor1_Model1\n3 sens        binary         0.135 Preprocessor1_Model1\n4 mn_log_loss binary         0.330 Preprocessor1_Model1\n5 roc_auc     binary         0.797 Preprocessor1_Model1\n\n\nVisualize the classification tree.\n\ndiabetes_tree_final_fit |&gt;\n  extract_workflow() |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = F, faclen = 2)\n\n\n\n\n\n\n\n\nProduce a variable importance plot.\n\ndiabetes_tree_final_model &lt;- extract_fit_engine(diabetes_tree_final_fit) \ndiabetes_tree_final_model |&gt;\n  vip(num_features = 15)\n\n\n\n\n\n\n\n\nGenerate a confusion matrix for final fit.\n\ntree_predictions &lt;- \n  diabetes_tree_final_fit |&gt;\n  collect_predictions()\n\ntree_conf_matrix &lt;-conf_mat(tree_predictions, truth = Diabetes, estimate = .pred_class)\ntree_conf_matrix\n\n          Truth\nPrediction   Yes    No\n       Yes  1434  1206\n       No   9170 64295\n\n\nCollect and save final metrics for best Classification Tree model.\n\ntree_metrics &lt;-\n  diabetes_tree_final_fit |&gt;\n  collect_metrics()\ntree_metrics\n\n# A tibble: 5 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.864 Preprocessor1_Model1\n2 spec        binary         0.982 Preprocessor1_Model1\n3 sens        binary         0.135 Preprocessor1_Model1\n4 mn_log_loss binary         0.330 Preprocessor1_Model1\n5 roc_auc     binary         0.797 Preprocessor1_Model1"
  },
  {
    "objectID": "Modeling.html#fit-random-forest-model",
    "href": "Modeling.html#fit-random-forest-model",
    "title": "Diabetes Modeling",
    "section": "Fit Random Forest model",
    "text": "Fit Random Forest model\nRandom Forest is an ensemble method that combines many decision trees using both bootstrapping and feature randomness. Bootstrapping involves creating multiple trees by randomly sampling from the training data set with replacement. Feature randomness involves selecting a random subset of predictors at each split. Compared to a single tree or bagged tree, feature randomness helps mitigate the influence of a strong predictor, or strong predictors, that might otherwise create highly correlated trees. In classification, a majority vote from all trees determines the final prediction.\nSet up Random Forest fit to use the “ranger” engine.\n\nrf_mod &lt;- rand_forest(mtry = tune(),\n                      min_n = 25,\n                      trees = 500) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\nCreate a workflow using the recipe and model defined earlier.\n\ndiabetes_rf_wfl &lt;- \n  workflow() |&gt;\n  add_recipe(diabetes_rec) |&gt;\n  add_model(rf_mod)\n\nCreate a tuning grid to consider varying levels of the number of predictors that will be randomly sampled at each node split (mtry) .\n\nplan(multisession, workers = 4)\n\nset.seed(558)\n\nrf_fits &lt;-\n  diabetes_rf_wfl |&gt; \n  tune_grid(resamples = diabetes_5_fold, grid = 5,\n            metrics = metric_set(mn_log_loss))\n\nplan(sequential)\n\nSort models by tuned mtry parameter with lowest log loss.\n\nrf_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     2 mn_log_loss binary     0.317     5 0.000830 Preprocessor1_Model2\n2     4 mn_log_loss binary     0.323     5 0.00120  Preprocessor1_Model5\n3     6 mn_log_loss binary     0.332     5 0.00177  Preprocessor1_Model1\n4     8 mn_log_loss binary     0.342     5 0.00200  Preprocessor1_Model3\n5     9 mn_log_loss binary     0.346     5 0.00206  Preprocessor1_Model4\n\n\nIdentify the parameter specification with lowest log loss to inform the best model.\n\nrf_best_params &lt;-\n  rf_fits |&gt;\n  select_best(metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     2 Preprocessor1_Model2\n\n\nUsing the best model, fit the model to the entire training data set using the last_fit() function. Compute the log loss and other performance metrics on the test set.\n\ndiabetes_rf_final_fit &lt;-\n  diabetes_rf_wfl |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  last_fit(split = diabetes_split, metrics = metrics_set)\n\ndiabetes_rf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 5 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.866 Preprocessor1_Model1\n2 spec        binary         0.989 Preprocessor1_Model1\n3 sens        binary         0.101 Preprocessor1_Model1\n4 mn_log_loss binary         0.317 Preprocessor1_Model1\n5 roc_auc     binary         0.824 Preprocessor1_Model1\n\n\nProduce a variable importance plot.\n\ndiabetes_rf_final_model &lt;- extract_fit_engine(diabetes_rf_final_fit) \ndiabetes_rf_final_model |&gt;\n  vip(num_features = 15)\n\n\n\n\n\n\n\n\nGenerate a confusion matrix for final fit.\n\nrf_predictions &lt;- \n  diabetes_tree_final_fit |&gt;\n  collect_predictions()\n\nrf_conf_matrix &lt;-conf_mat(rf_predictions, truth = Diabetes, estimate = .pred_class)\nrf_conf_matrix\n\n          Truth\nPrediction   Yes    No\n       Yes  1434  1206\n       No   9170 64295\n\n\nCollect and save final metrics for best Random Forest model.\n\nrf_metrics &lt;-\n  diabetes_rf_final_fit |&gt;\n  collect_metrics()\nrf_metrics\n\n# A tibble: 5 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.866 Preprocessor1_Model1\n2 spec        binary         0.989 Preprocessor1_Model1\n3 sens        binary         0.101 Preprocessor1_Model1\n4 mn_log_loss binary         0.317 Preprocessor1_Model1\n5 roc_auc     binary         0.824 Preprocessor1_Model1"
  },
  {
    "objectID": "Modeling.html#select-the-best-model",
    "href": "Modeling.html#select-the-best-model",
    "title": "Diabetes Modeling",
    "section": "Select the best model",
    "text": "Select the best model\nCompare all final models using both log loss and ROC AUC.\n\nrbind(tree_metrics, rf_metrics) |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  mutate(model = c(\"Classification Tree\", \"Random Forest\")) |&gt;\n  select(model, \"mean_mn_log_loss\" = .estimate)\n\n# A tibble: 2 × 2\n  model               mean_mn_log_loss\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 Classification Tree            0.330\n2 Random Forest                  0.317\n\n\n\nrbind(tree_metrics, rf_metrics) |&gt;\n  filter(.metric == \"roc_auc\") |&gt;\n  mutate(model = c(\"Classification Tree\", \"Random Forest\")) |&gt;\n  select(model, \"mean_roc_auc\" = .estimate)\n\n# A tibble: 2 × 2\n  model               mean_roc_auc\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Classification Tree        0.797\n2 Random Forest              0.824\n\n\nThe Random Forest model performs slightly better on both metrics so that will serve as our final model.\nSave the best model for use in the API.\n\nsave(diabetes_rf_final_model, file = \"model/diabetes_rf_final_model.RData\")\nsave(diabetes_rf_final_fit, file = \"model/diabetes_rf_final_fit.RData\")"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Diabetes EDA",
    "section": "",
    "text": "This project considers the Diabetes Health Indicators data set. The data is drawn from a 2015 Behavioral Risk Factor Surveillance System (BRFSS) survey conducted by the Centers for Disease Control and Prevention (CDC).\nThis investigation and analysis considers whether answers to these survey questions might prove useful in predicting a diabetes diagnosis, and, if so, which subset of potential predictors should be considered in modeling.\nThere are 253,680 observations in the data set and 21 potential predictors.\nAdditional information on the data set and survey can be found via the links below:\nDiabetes Dataset\nCodebook for Variables\n\n\nVariables in the data set include:\nDiabetes: whether the individual received a diabetes (or prediabetes) diagnosis (“Yes” or “No”) this is the response variable\nHighBP: whether or not the individual had ever been diagnosed with high blood pressure (“Yes” or “No”)\nHighChol: whether or not the individual had ever been diagnosed with high cholesterol (“Yes” or “No”)\nCholCheck: whether or not the individual has had cholesterol levels checked within the past five years (“Yes” or “No”)\nBMI: body mass index of the individual (numeric value)\nSmoker: whether or not the individual had smoked at least 100 cigarettes over entire life (“Yes” or “No”)\nStroke: whether or not the individual had ever had a stroke (“Yes” or “No”)\nHeartDiseaseorAttack: whether or not the individual had ever had coronary heart disease or myocardial infarction (“Yes” or “No”)\nPhysActivity: whether or not the individual had been physically active in the prior 30 days (“Yes” or “No”)\nFruits: whether or not the individual had consumed fruit one or more times per day in the prior 30 days (“Yes” or “No”)\nVeggies: whether or not the individual had consumed vegetables one or more times per day in the prior 30 days (“Yes” or “No”)\nHvyAlcoholConsump: whether or not the individual would be considered a heavy drinker based upon reported alcohol consumption behaviors over the prior 30 days (“Yes” or “No”)\nAnyHealthcare: whether or not the individual had any form of healthcare coverage (“Yes” or “No”)\nNoDocbcCost: whether or not the individual had at any time during the prior 12 months needed to see a doctor but could not because of cost (“Yes” or “No”)\nGenHlth: the individual’s self-assessment of his or her general health (from 1 for excellent to 5 for poor)\nMentHlth: days of poor mental health over the prior 30 days (numeric value)\nPhysHlth: days of poor physical illness or injury over the prior 30 days (numeric value)\nDiffWalk: whether or not the individual faced serious difficulty walking or climbing stairs (“Yes” or “No”)\nSex: indicated sex of respondent (“Female” or “Male”)\nAge: indicated age group of respondent (“18 to 24”, “25 to 29”, “30 to 34”, “35 to 39”, “40 to 44”, “45 to 49”, “50 to 54”, “55 to 59”, “60 to 64”, “65 to 69”, “70 to 74”, “75 to 79”, “80 or older”)\nEducation: indicated highest grade or year of school completed (“No school or only kindergarten”, “Elementary”, “Some high school”, “High school graduate”, “Some college or technical school”, “College graduate”)\nIncome: indicated household income group (“Less than $10,000”, “$10,000 to less than $15,000”, “$15,000 to less than $20,000”, “$20,000 to less than $25,000”, “$25,000 to less than $35,000”, “$35,000 to less than $50,000”, “$50,000 to less than $75,000”, “$75,000 or more”)\n\n\n\nThe purpose of EDA is to:\n\nunderstand how the data is stored\nperform some basic data validation\ndetermine the rate of missing values\nclean up the data as needed\ninvestigate distributions and summary statistics\napply any transformations to improve efficacy of the data set for further analysis and modeling"
  },
  {
    "objectID": "EDA.html#introduction-and-overview",
    "href": "EDA.html#introduction-and-overview",
    "title": "Diabetes EDA",
    "section": "",
    "text": "This project considers the Diabetes Health Indicators data set. The data is drawn from a 2015 Behavioral Risk Factor Surveillance System (BRFSS) survey conducted by the Centers for Disease Control and Prevention (CDC).\nThis investigation and analysis considers whether answers to these survey questions might prove useful in predicting a diabetes diagnosis, and, if so, which subset of potential predictors should be considered in modeling.\nThere are 253,680 observations in the data set and 21 potential predictors.\nAdditional information on the data set and survey can be found via the links below:\nDiabetes Dataset\nCodebook for Variables\n\n\nVariables in the data set include:\nDiabetes: whether the individual received a diabetes (or prediabetes) diagnosis (“Yes” or “No”) this is the response variable\nHighBP: whether or not the individual had ever been diagnosed with high blood pressure (“Yes” or “No”)\nHighChol: whether or not the individual had ever been diagnosed with high cholesterol (“Yes” or “No”)\nCholCheck: whether or not the individual has had cholesterol levels checked within the past five years (“Yes” or “No”)\nBMI: body mass index of the individual (numeric value)\nSmoker: whether or not the individual had smoked at least 100 cigarettes over entire life (“Yes” or “No”)\nStroke: whether or not the individual had ever had a stroke (“Yes” or “No”)\nHeartDiseaseorAttack: whether or not the individual had ever had coronary heart disease or myocardial infarction (“Yes” or “No”)\nPhysActivity: whether or not the individual had been physically active in the prior 30 days (“Yes” or “No”)\nFruits: whether or not the individual had consumed fruit one or more times per day in the prior 30 days (“Yes” or “No”)\nVeggies: whether or not the individual had consumed vegetables one or more times per day in the prior 30 days (“Yes” or “No”)\nHvyAlcoholConsump: whether or not the individual would be considered a heavy drinker based upon reported alcohol consumption behaviors over the prior 30 days (“Yes” or “No”)\nAnyHealthcare: whether or not the individual had any form of healthcare coverage (“Yes” or “No”)\nNoDocbcCost: whether or not the individual had at any time during the prior 12 months needed to see a doctor but could not because of cost (“Yes” or “No”)\nGenHlth: the individual’s self-assessment of his or her general health (from 1 for excellent to 5 for poor)\nMentHlth: days of poor mental health over the prior 30 days (numeric value)\nPhysHlth: days of poor physical illness or injury over the prior 30 days (numeric value)\nDiffWalk: whether or not the individual faced serious difficulty walking or climbing stairs (“Yes” or “No”)\nSex: indicated sex of respondent (“Female” or “Male”)\nAge: indicated age group of respondent (“18 to 24”, “25 to 29”, “30 to 34”, “35 to 39”, “40 to 44”, “45 to 49”, “50 to 54”, “55 to 59”, “60 to 64”, “65 to 69”, “70 to 74”, “75 to 79”, “80 or older”)\nEducation: indicated highest grade or year of school completed (“No school or only kindergarten”, “Elementary”, “Some high school”, “High school graduate”, “Some college or technical school”, “College graduate”)\nIncome: indicated household income group (“Less than $10,000”, “$10,000 to less than $15,000”, “$15,000 to less than $20,000”, “$20,000 to less than $25,000”, “$25,000 to less than $35,000”, “$35,000 to less than $50,000”, “$50,000 to less than $75,000”, “$75,000 or more”)\n\n\n\nThe purpose of EDA is to:\n\nunderstand how the data is stored\nperform some basic data validation\ndetermine the rate of missing values\nclean up the data as needed\ninvestigate distributions and summary statistics\napply any transformations to improve efficacy of the data set for further analysis and modeling"
  },
  {
    "objectID": "EDA.html#load-required-packages",
    "href": "EDA.html#load-required-packages",
    "title": "Diabetes EDA",
    "section": "Load required packages",
    "text": "Load required packages\n\nlibrary(conflicted)\nlibrary(skimr)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggmosaic)\nlibrary(knitr)\n\nconflicts_prefer(dplyr::lag)\nconflicts_prefer(dplyr::filter)\n\noptions(scipen = 999, digits = 2)"
  },
  {
    "objectID": "EDA.html#read-data",
    "href": "EDA.html#read-data",
    "title": "Diabetes EDA",
    "section": "Read data",
    "text": "Read data\nRead in data from local drive (originally sourced from Kaggle).\n\ndiabetes_raw &lt;- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")"
  },
  {
    "objectID": "EDA.html#check-and-manipulate-the-data",
    "href": "EDA.html#check-and-manipulate-the-data",
    "title": "Diabetes EDA",
    "section": "Check and manipulate the data",
    "text": "Check and manipulate the data\nExplore columns names, column types, and values.\n\nglimpse(diabetes_raw)\n\nRows: 253,680\nColumns: 22\n$ Diabetes_binary      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0…\n$ HighBP               &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1…\n$ HighChol             &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ CholCheck            &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0…\n$ Stroke               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ HeartDiseaseorAttack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PhysActivity         &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ Fruits               &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1…\n$ Veggies              &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ HvyAlcoholConsump    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ AnyHealthcare        &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ NoDocbcCost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ GenHlth              &lt;dbl&gt; 5, 3, 5, 2, 2, 2, 3, 3, 5, 2, 3, 3, 3, 4, 4, 2, 3…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n$ Sex                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ Age                  &lt;dbl&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11, …\n$ Education            &lt;dbl&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6, 4…\n$ Income               &lt;dbl&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8, 3…\n\n\nAll columns are reading in as double type. This does not align with the variable descriptions in the introduction. We will address this shortly.\nCheck for any missing values.\n\ndiabetes_raw |&gt;\n  skim() |&gt;\n  focus(n_missing, complete_rate, numeric.hist)\n\n\nData summary\n\n\nName\ndiabetes_raw\n\n\nNumber of rows\n253680\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n22\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nhist\n\n\n\n\nDiabetes_binary\n0\n1\n▇▁▁▁▁\n\n\nHighBP\n0\n1\n▇▁▁▁▆\n\n\nHighChol\n0\n1\n▇▁▁▁▆\n\n\nCholCheck\n0\n1\n▁▁▁▁▇\n\n\nBMI\n0\n1\n▇▅▁▁▁\n\n\nSmoker\n0\n1\n▇▁▁▁▆\n\n\nStroke\n0\n1\n▇▁▁▁▁\n\n\nHeartDiseaseorAttack\n0\n1\n▇▁▁▁▁\n\n\nPhysActivity\n0\n1\n▂▁▁▁▇\n\n\nFruits\n0\n1\n▅▁▁▁▇\n\n\nVeggies\n0\n1\n▂▁▁▁▇\n\n\nHvyAlcoholConsump\n0\n1\n▇▁▁▁▁\n\n\nAnyHealthcare\n0\n1\n▁▁▁▁▇\n\n\nNoDocbcCost\n0\n1\n▇▁▁▁▁\n\n\nGenHlth\n0\n1\n▅▇▇▃▁\n\n\nMentHlth\n0\n1\n▇▁▁▁▁\n\n\nPhysHlth\n0\n1\n▇▁▁▁▁\n\n\nDiffWalk\n0\n1\n▇▁▁▁▂\n\n\nSex\n0\n1\n▇▁▁▁▆\n\n\nAge\n0\n1\n▂▃▇▇▆\n\n\nEducation\n0\n1\n▁▁▅▅▇\n\n\nIncome\n0\n1\n▁▁▃▂▇\n\n\n\n\n\nThere are no missing or values outside the ranges outlined in the variable descriptions in the introduction. The data appears to be clean. The mini-histogram confirms most of the variables to be binary. The Mental Health and Physical Health variables look to be highly skewed. We will investigate this further.\nAssign a vector for the numeric variables as described in the introduction: BMI, MentHlth, and PhysHlth. Consistent with social science research practices, we will treat the GenHlth Likert-scale question as interval data (although technically ordinal), and, as such, include among the numeric variables. Assign a vector for the multilevel factors as described in the introduction: Age, Education, and Income.\n\ntrue_numeric_cols &lt;- c(\"BMI\", \"GenHlth\", \"MentHlth\", \"PhysHlth\")\nmulti_fac_cols &lt;- c(\"Age\", \"Education\", \"Income\")\n\nRename the Diabetes_binary variable to Diabetes for simplicity. Create factors for binary variables: Sex and all the “Yes”/“No” variables described in the introduction.\n\ndiabetes &lt;-\n  diabetes_raw |&gt;\n  rename(Diabetes = Diabetes_binary) |&gt;\n  mutate(Sex = factor(Sex, levels = 0:1, labels = c(\"Female\", \"Male\"))) |&gt;\n  mutate(across(\n    .cols = !c(Sex, all_of(true_numeric_cols), all_of(multi_fac_cols)),\n    .fns = ~ factor(.x, levels = 0:1, labels = c(\"No\", \"Yes\"))\n  )) |&gt;\n  mutate(across(where(is.factor), fct_rev)) #so modeling correctly reflects positive class\n\nCreate ordered factors for the multilevel variables: Age, Education, and Income.\n\ndiabetes &lt;-\n  diabetes |&gt;\n  mutate(Age = factor(Age, \n                      levels = 1:13, \n                      labels = c(\"18 to 24\", \"25 to 29\", \"30 to 34\", \n                                 \"35 to 39\", \"40 to 44\", \"45 to 49\", \n                                 \"50 to 54\", \"55 to 59\", \"60 to 64\", \n                                 \"65 to 69\", \"70 to 74\", \"75 to 79\", \n                                 \"80 or older\"),\n                      ordered = T)) |&gt;\n  mutate(Education = factor(Education, \n                            levels = 1:6, \n                            labels = c(\"No school or only kindergarten\", \n                                       \"Elementary\", \n                                       \"Some high school\", \n                                       \"High school graduate\", \n                                       \"Some college or technical school\", \n                                       \"College graduate\"),\n                            ordered = T)) |&gt;\n  mutate(Income = factor(Income, \n                         levels = 1:8, \n                         labels = c(\"Less than $10,000\", \n                                    \"$10,000 to less than $15,000\", \n                                    \"$15,000 to less than $20,000\", \n                                    \"$20,000 to less than $25,000\", \n                                    \"$25,000 to less than $35,000\", \n                                    \"$35,000 to less than $50,000\", \n                                    \"$50,000 to less than $75,000\", \n                                    \"$75,000 or more\"),\n                         ordered = T))\n\nConfirm changes to variable types.\n\nglimpse(diabetes)\n\nRows: 253,680\nColumns: 22\n$ Diabetes             &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, No, Yes, No,…\n$ HighBP               &lt;fct&gt; Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, N…\n$ HighChol             &lt;fct&gt; Yes, No, Yes, No, Yes, Yes, No, Yes, Yes, No, No,…\n$ CholCheck            &lt;fct&gt; Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, …\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;fct&gt; Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, No, Yes…\n$ Stroke               &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ HeartDiseaseorAttack &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, No, No, No, …\n$ PhysActivity         &lt;fct&gt; No, Yes, No, Yes, Yes, Yes, No, Yes, No, No, Yes,…\n$ Fruits               &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, No, No, Yes, No, Yes,…\n$ Veggies              &lt;fct&gt; Yes, No, No, Yes, Yes, Yes, No, Yes, Yes, Yes, Ye…\n$ HvyAlcoholConsump    &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ AnyHealthcare        &lt;fct&gt; Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, …\n$ NoDocbcCost          &lt;fct&gt; No, Yes, Yes, No, No, No, No, No, No, No, No, No,…\n$ GenHlth              &lt;dbl&gt; 5, 3, 5, 2, 2, 2, 3, 3, 5, 2, 3, 3, 3, 4, 4, 2, 3…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;fct&gt; Yes, No, Yes, No, No, No, No, Yes, Yes, No, No, Y…\n$ Sex                  &lt;fct&gt; Female, Female, Female, Female, Female, Male, Fem…\n$ Age                  &lt;ord&gt; 60 to 64, 50 to 54, 60 to 64, 70 to 74, 70 to 74,…\n$ Education            &lt;ord&gt; High school graduate, College graduate, High scho…\n$ Income               &lt;ord&gt; \"$15,000 to less than $20,000\", \"Less than $10,00…\n\n\nGenerate basic summary statistics for numeric columns.\n\ndiabetes |&gt;\n  select(where(is.numeric)) |&gt;\n  describe()\n\n         vars      n mean  sd median trimmed mad min max range skew kurtosis\nBMI         1 253680 28.4 6.6     27    27.7 4.5  12  98    86 2.12    11.00\nGenHlth     2 253680  2.5 1.1      2     2.5 1.5   1   5     4 0.42    -0.38\nMentHlth    3 253680  3.2 7.4      0     1.0 0.0   0  30    30 2.72     6.44\nPhysHlth    4 253680  4.2 8.7      0     1.8 0.0   0  30    30 2.21     3.50\n           se\nBMI      0.01\nGenHlth  0.00\nMentHlth 0.01\nPhysHlth 0.02\n\n\nThe median values of zero, skew of &gt;2, and low mean values confirm the Mental Health and Physical Health variables to deviate from normality. General Health, however, appears to be approximately normal. BMI appears to be right skewed, but the max value of 98 would indicate extreme, severe obesity. This value is highly unusual and worth further exploration.\nCheck count and percentage of BMI values indicating severe obesity.\n\ndiabetes |&gt;\n  select(BMI) |&gt;\n  filter(BMI &gt; 50) |&gt;\n  mutate(BMI_cut = cut(BMI, breaks = c(50, 60, 70, 80, 90, 100))) |&gt;\n  group_by(BMI_cut) |&gt;\n  summarise(count = n()) |&gt;\n  mutate(percent = count / length(diabetes$BMI) * 100)\n\n# A tibble: 5 × 3\n  BMI_cut  count percent\n  &lt;fct&gt;    &lt;int&gt;   &lt;dbl&gt;\n1 (50,60]   1370  0.540 \n2 (60,70]    221  0.0871\n3 (70,80]    305  0.120 \n4 (80,90]    226  0.0891\n5 (90,100]    53  0.0209\n\n\nThe number of observations with BMI above 50 is surprising (and those above 80 especially so). That said, as they cumulatively represent less than 1% of total observations and we have no strong rationale for removing, we will leave these outliers in the data set.\nCheck unique values for the categorical variables.\n\ndiabetes |&gt;\n  select(where(is.factor)) |&gt;\n  summary(maxsum = Inf)\n\n Diabetes     HighBP       HighChol     CholCheck    Smoker       Stroke      \n Yes: 35346   Yes:108829   Yes:107591   Yes:244210   Yes:112423   Yes: 10292  \n No :218334   No :144851   No :146089   No :  9470   No :141257   No :243388  \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n HeartDiseaseorAttack PhysActivity Fruits       Veggies      HvyAlcoholConsump\n Yes: 23893           Yes:191920   Yes:160898   Yes:205841   Yes: 14256       \n No :229787           No : 61760   No : 92782   No : 47839   No :239424       \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n AnyHealthcare NoDocbcCost  DiffWalk         Sex                  Age       \n Yes:241263    Yes: 21354   Yes: 42675   Male  :111706   18 to 24   : 5700  \n No : 12417    No :232326   No :211005   Female:141974   25 to 29   : 7598  \n                                                         30 to 34   :11123  \n                                                         35 to 39   :13823  \n                                                         40 to 44   :16157  \n                                                         45 to 49   :19819  \n                                                         50 to 54   :26314  \n                                                         55 to 59   :30832  \n                                                         60 to 64   :33244  \n                                                         65 to 69   :32194  \n                                                         70 to 74   :23533  \n                                                         75 to 79   :15980  \n                                                         80 or older:17363  \n                            Education                               Income     \n No school or only kindergarten  :   174   Less than $10,000           : 9811  \n Elementary                      :  4043   $10,000 to less than $15,000:11783  \n Some high school                :  9478   $15,000 to less than $20,000:15994  \n High school graduate            : 62750   $20,000 to less than $25,000:20135  \n Some college or technical school: 69910   $25,000 to less than $35,000:25883  \n College graduate                :107325   $35,000 to less than $50,000:36470  \n                                           $50,000 to less than $75,000:43219  \n                                           $75,000 or more             :90385  \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n\n\nSave transformed data set for use in modeling phase.\n\nsaveRDS(diabetes, \"data/diabetes.rds\")\nsave(diabetes, file = \"data/diabetes.RData\")\n\nAs we continue Exploratory Data Analysis and Modeling, the following variables are excluded from consideration based upon early stage modeling which revealed them to have lower variable importance in many decision tree models: CholCheck, Smoker, Stroke, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, DiffWalk, and Education. To retain the integrity of the full data set, these variables are not removed in the saved data set. Instead, step_rm is used during recipe definition in the Modeling phase to exclude these variables."
  },
  {
    "objectID": "EDA.html#investigate-and-summarize-distributions",
    "href": "EDA.html#investigate-and-summarize-distributions",
    "title": "Diabetes EDA",
    "section": "Investigate and summarize distributions",
    "text": "Investigate and summarize distributions\nDetermine frequency of diabetes diagnoses across the data set.\n\ndiabetes |&gt;\n  ggplot(aes(x = Diabetes)) +\n  geom_bar(aes(fill = Diabetes)) +\n  theme(legend.position='none') +\n  scale_fill_manual(values = c(\"Yes\"=\"#F51304\",\"No\"=\"#2D8AEB\"))\n\n\n\n\n\n\n\n\nDetermine the percentage of diabetes diagnoses across the data set.\n\ndiabetes |&gt;\n  tabyl(Diabetes) |&gt;\n  adorn_totals(\"row\") |&gt;\n  adorn_pct_formatting()\n\n Diabetes      n percent\n      Yes  35346   13.9%\n       No 218334   86.1%\n    Total 253680  100.0%\n\n\nThe positive class (i.e., “Yes” on Diabetes) appears in less than 14% of cases. This suggests we may wish to consider stratification in the train-test split and cross-validation folds.\nExplore the percentage of diabetes diagnoses relative to the sex of the respondent.\n\ndiabetes |&gt;\n  tabyl(Sex, Diabetes) |&gt;\n  adorn_percentages() |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_title()\n\n        Diabetes      \n    Sex      Yes    No\n   Male    15.2% 84.8%\n Female    13.0% 87.0%\n\n\nThe relative occurrence of diabetes diagnoses does not appear to differ drastically based on sex. By itself, the Sex variable is unlikely to be a strong predictor of Diabetes, but perhaps useful in combination with other variables.\nExplore the percentage of diabetes diagnoses relative to whether or not the respondent had ever been diagnosed with high blood pressure.\n\ndiabetes |&gt;\n  tabyl(HighBP, Diabetes) |&gt;\n  adorn_percentages() |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_title()\n\n        Diabetes      \n HighBP      Yes    No\n    Yes    24.4% 75.6%\n     No     6.0% 94.0%\n\n\nGiven the percentage “Yes” for high blood pressure is quite a bit higher than the 14% found in the one-way contingency table on Diabetes reported earlier, this variable may help us predict a positive diagnosis.\n\ndiabetes |&gt;\n  tabyl(HighChol, Diabetes) |&gt;\n  adorn_percentages() |&gt;\n  adorn_pct_formatting() |&gt;\n  adorn_title()\n\n          Diabetes      \n HighChol      Yes    No\n      Yes    22.0% 78.0%\n       No     8.0% 92.0%\n\n\nSimilarly, the percentage “Yes” for high cholesterol is quite a bit higher than the 14% in the one-way contingency table on Diabetes reported earlier. This variable also may help us predict a positive diagnosis.\nCheck how correlated the numeric variables are with one another.\n\ndiabetes |&gt;\n  select(where(is.numeric)) |&gt;\n  cor()\n\n           BMI GenHlth MentHlth PhysHlth\nBMI      1.000    0.24    0.085     0.12\nGenHlth  0.239    1.00    0.302     0.52\nMentHlth 0.085    0.30    1.000     0.35\nPhysHlth 0.121    0.52    0.354     1.00\n\n\nFor most combinations, there appears to be only weak correlation. The highest is GenHlth with PhysHlth, but this is still only a medium-strength correlation. This suggests we would not be particularly concerned with multicollinearity across these numeric variables.\nGet summary statistics grouped by diabetes diagnosis.\n\ndiabetes |&gt;\n  group_by(Diabetes) |&gt;\n  summarise(across(where(is.numeric),\n                   list(\"median\" = median, \"mean\" = mean, \"stdev\" = sd),\n                   .names = \"{.fn}_{.col}\"))|&gt;\n  mutate(across(where(is.numeric), round, 2)) |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiabetes\nmedian_BMI\nmean_BMI\nstdev_BMI\nmedian_GenHlth\nmean_GenHlth\nstdev_GenHlth\nmedian_MentHlth\nmean_MentHlth\nstdev_MentHlth\nmedian_PhysHlth\nmean_PhysHlth\nstdev_PhysHlth\n\n\n\n\nYes\n31\n32\n7.4\n3\n3.3\n1\n0\n4.5\n8.9\n1\n8.0\n11.3\n\n\nNo\n27\n28\n6.3\n2\n2.4\n1\n0\n3.0\n7.1\n0\n3.6\n8.1\n\n\n\n\n\nWithout running any tests of statistical significance, we can tentatively observe that the mean for the general health rating (GenHlth) and number of days of poor physical health (PhysHlth) looks to be higher for those with a positive diabetes diagnosis.\nInvestigate distribution of diabetes diagnosis by BMI.\n\ndiabetes |&gt;\n  ggplot(aes(Diabetes, BMI)) +\n  geom_violin(aes(fill = Diabetes)) +\n  stat_summary(fun = \"mean\",\n               geom = \"crossbar\", \n               width = 0.5,\n               linewidth = 0.2,\n               colour = \"white\") +\n  theme(legend.position = \"none\") +\n  xlab(\"Diabetes\") +\n  ylab(\"BMI\") +\n  ggtitle(\"Distribution of Diabetes Diagnosis by BMI\") +\n  scale_fill_manual(values = c(\"Yes\"=\"#F51304\",\"No\"=\"#2D8AEB\"))\n\n\n\n\n\n\n\n\nA diabetes diagnosis (Diabetes) of “No” appears to be more tightly distributed around a lower mean. A quick t-test confirms this to a statistically significant difference. High BMI may help us predict a positive diagnosis.\n\nBMI_Yes &lt;- diabetes |&gt; filter(Diabetes == \"Yes\") |&gt; pull(BMI)\nBMI_No &lt;- diabetes |&gt; filter(Diabetes == \"No\") |&gt; pull(BMI)\n\nt.test(BMI_Yes, BMI_No, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  BMI_Yes and BMI_No\nt = 112, df = 253678, p-value &lt;0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 4.1 4.2\nsample estimates:\nmean of x mean of y \n       32        28 \n\n\nCompare the density plot of days of poor physical health (PhysHlth) for positive versus negative diabetes diagnosis.\n\ndiabetes |&gt;\n  ggplot(aes(PhysHlth)) +\n  geom_density() +\n  theme(legend.position = \"none\") +\n  xlab(\"Physical Health\") +\n  ggtitle(\"Distribution of Diabetes Diagnosis by Physical Health\") +\n  facet_wrap(~ Diabetes)\n\n\n\n\n\n\n\n\nThe “No” class for Diabetes is highly peaked a 0, while the “Yes” class looks to be a bit more uniformly distributed. The PhysHlth variable seems like a good candidate to include in our prediction model.\n\nPhys_Yes &lt;- diabetes |&gt; filter(Diabetes == \"Yes\") |&gt; pull(PhysHlth)\nPhys_No &lt;- diabetes |&gt; filter(Diabetes == \"No\") |&gt; pull(PhysHlth)\n\nt.test(Phys_Yes, Phys_No, var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  Phys_Yes and Phys_No\nt = 69, df = 41367, p-value &lt;0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 4.2 4.4\nsample estimates:\nmean of x mean of y \n      8.0       3.6 \n\n\nCompare the density plot of days of poor mental health (MentHlth) for positive versus negative diabetes diagnosis.\n\ndiabetes |&gt;\n  ggplot(aes(MentHlth)) +\n  geom_density() +\n  theme(legend.position = \"none\") +\n  xlab(\"Mental Health\") +\n  ggtitle(\"Distribution of Diabetes Diagnosis by Mental Health\") +\n  facet_wrap(~ Diabetes)\n\n\n\n\n\n\n\n\nThe visual distinction between classes of Diabetes is less pronounced for days of poor mental health (MentHlth) than for physical health (PhysHlth). That said, perhaps in conjunction with other variables, MentHlth can be useful is our prediction model.\nConsider a histogram of general health ratings (GenHlth) highlighting counts of the “Yes” class of Diabetes compared to the “No” class.\n\ndiabetes |&gt;\n  ggplot(aes(GenHlth)) +\n  geom_histogram(bins = 5, aes(fill = Diabetes), position=\"identity\", alpha=0.5) +\n  xlab(\"General Health\") +\n  ggtitle(\"Distribution of Diabetes Diagnosis by General Health\") +\n  scale_fill_manual(values = c(\"Yes\"=\"#F51304\",\"No\"=\"#2D8AEB\"))\n\n\n\n\n\n\n\n\nIndividuals with a positive diagnosis appear more likely to have self-assessed their general health toward the “poor” end of the spectrum (i.e., toward “5”). The GenHlth variable should be useful in our modeling exercise.\nPrepare a mosaic plot to investigate how the prevalence of a positive diabetes diagnosis might differ by age group categories.\n\ndiabetes |&gt;\n  ggplot() +\n  geom_mosaic(aes(x = product(Age), fill = Diabetes)) +\n  theme(legend.position = \"none\") +\n  xlab(\"Age Group\") +\n  ylab(\"Diabetes\") +\n  ggtitle(\"Prevalence of Diabetes Diagnosis by Age Group\") +\n  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1)) +\n  scale_fill_manual(values = c(\"Yes\"=\"#F51304\",\"No\"=\"#2D8AEB\"))\n\n\n\n\n\n\n\n\nThere seems to be an association between the “Yes” class of Diabetes and higher age groups (Age). The proportion of “Yes” is greatest for “70 to 74”, although generally it seems to climb with age and roughly plateau after 65.\nPrepare a mosaic plot to investigate how the prevalence of a positive diabetes diagnosis might differ by household income group categories.\n\ndiabetes |&gt;\n  ggplot() +\n  geom_mosaic(aes(x = product(Income), fill = Diabetes)) +\n  theme(legend.position = \"none\") +\n  xlab(\"Income Range\") +\n  ylab(\"Diabetes\") +\n  ggtitle(\"Prevalence of Diabetes Diagnosis by Income Range\") +\n  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1)) +\n  scale_fill_manual(values = c(\"Yes\"=\"#F51304\",\"No\"=\"#2D8AEB\"))\n\n\n\n\n\n\n\n\nThe “Yes” class of Diabetes seems to be more common among lower income groups (Income)."
  },
  {
    "objectID": "EDA.html#modeling",
    "href": "EDA.html#modeling",
    "title": "Diabetes EDA",
    "section": "Modeling",
    "text": "Modeling\nNext we carry these findings into an effort to build a model which could help us predict a positive diabetes diagnosis.\nFor results of the modeling exercise, go here: Modeling\nThe story continues…"
  }
]