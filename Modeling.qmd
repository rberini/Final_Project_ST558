---
title: "Diabetes Modeling"
author: "Robert Berini"
format: html
editor: visual
execute: 
  warning: false
  message: false
---

## Introduction and overview

In this phase, we turn our attention to the process of fitting and selecting a best model to predict a diabetes diagnosis using the subset of variables described during our Exploratory Data Analysis (EDA) phase.

As outlined in the corresponding [EDA report](https://rberini.github.io/Final_Project_ST558/EDA.html "EDA"), this project considers the Diabetes Health Indicators data set. The data is drawn from a 2015 Behavioral Risk Factor Surveillance System (BRFSS) survey conducted by the Centers for Disease Control and Prevention (CDC).

There are 253,680 observations in the data set and 21 potential predictors.

This investigation and analysis considers whether answers to the BRFSS survey questions might prove useful in predicting a diabetes diagnosis, and, if so, which subset of potential predictors should be considered in modeling. The goal for this **modeling** phase is to create models for predicting the `Diabetes` variable. We use the `tidymodels` package to consider two models: a **classification tree** and a **random forest**. We will use **log loss** as our metric to evaluate the models. For both model types, we use log loss with 5 fold cross-validation to select the best model from that family of models. We then compare the best model from each family to select one best final model to be published via API.

## Load required packages

```{r}
library(conflicted)
library(tidyverse)
library(ranger) #not directly called; used by tidymodels package
library(tidymodels)
library(baguette) #not used; loading in case we later might consider a bagged tree
library(vip)
library(future) #attempting to improve tuning time through parallelization
library(furrr) #not directly called; used by future package

conflicts_prefer(dplyr::lag)
conflicts_prefer(dplyr::filter)
tidymodels_prefer()

options(scipen = 999, digits = 2)
```

## Read data

Load manipulated data set coming out of EDA phase.

```{r}
diabetes <- readRDS("data/diabetes.rds")
diabetes
```

## Split the data

Split the data into training and test sets (70/30 split). Use the `strata` argument to stratify the split on the `diabetes` outcome given the low proportion of positive cases in the data set.

```{r}
set.seed(558)
diabetes_split <- initial_split(diabetes, prop = 0.7, strata = Diabetes)
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)
```

For the training set, create a five fold cross-validation (CV) split. Again, we use the `strata` argument to stratify the folds on the `diabetes` outcome to appropriately reflect the training-test split. The five fold cross-validation method randomly splits the data set into five equal parts. Four of the splits are used to train the model and the fifth, a holdout or validation set, is used to evaluate performance based on the chosen metric (log loss in this case). Which specific fold is used for the validation set rotates among the five folds at each run. This method can be used to assess performance of the model as applied to data not used to train that model (i.e., the validation fold). In addition, when used with the tuning grid, this method can be used to estimate the best parameters.

```{r}
set.seed(558)
diabetes_5_fold <- vfold_cv(diabetes_train, v = 5, strata = Diabetes)
```

## Create recipe

Construct a **recipe** that defines the preprocessing steps for the models to follow. The recipe specifies the response variable (`Diabetes`) and which predictors to keep and remove (as outlined in the EDA phase). We opt not to normalize numeric predictors because decision trees are invariant to scaling.

```{r}
diabetes_rec <-
  recipe(Diabetes ~ ., data = diabetes_train) |>
  step_rm(CholCheck, Smoker, Stroke, PhysActivity:NoDocbcCost, DiffWalk, Education) #|>
  #step_normalize(all_numeric_predictors()) this step removed as not needed for trees
```

After creating the recipe, inspect the transformed training data.

```{r}
diabetes_rec |>
  prep(training = diabetes_train) |>
  bake(diabetes_train)
```

Define the metrics set. Our primary metric for training and evaluation is **log loss**; however, we may wish to consider other measures of performance. So here we also consider accuracy, specificity (i.e., true positive rate), sensitivity (i.e., true negative rate), and receiver operating characteristic area under the curve (representing how well the model can distinguish between positive and negative cases).

```{r}
metrics_set <- metric_set(mn_log_loss, roc_auc, accuracy, spec, sens)
```

## Fit Classification Tree model

First, we consider the Classification Tree family of models. **Classification Trees** are type of decision tree that start with a root node. Observations are then split at subsequent decision nodes using proportional weights to classify observations by predictor variables at each node. This eventually leads to leaf nodes composed of similar observations, which represent outcomes. Classification trees are employed for categorical response variables. They offer clear, visual representations of decisions, making it easy to understand and explain the classification process.

Set up the Classification Tree fit to use the `“rpart”` engine.

```{r}
tree_mod <- decision_tree(tree_depth = 8,
                          min_n = 25,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Create a workflow using the recipe and model defined earlier.

```{r}
diabetes_tree_wfl <- 
  workflow() |>
  add_recipe(diabetes_rec) |>
  add_model(tree_mod)
```

Create a tuning grid to consider varying levels of cost complexity.

```{r}
plan(multisession, workers = 4)

set.seed(558)

tree_fits <-
  diabetes_tree_wfl |> 
  tune_grid(resamples = diabetes_5_fold, grid = 20,
            metrics = metric_set(mn_log_loss))

plan(sequential)
```

Sort models by the tuned cost complexity parameter associated with lowest **log loss**.

```{r}
tree_fits |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

Identify the parameter specification with lowest **log loss** to inform the best model.

```{r}
tree_best_params <-
  tree_fits |>
  select_best(metric = "mn_log_loss")
tree_best_params
```

Using the best model, fit the model to the entire training data set using the `last_fit()` function. Compute the **log loss** and other performance metrics on the test set.

```{r}
diabetes_tree_final_fit <-
  diabetes_tree_wfl |>
  finalize_workflow(tree_best_params) |>
  last_fit(split = diabetes_split, metrics = metrics_set)

diabetes_tree_final_fit |>
  collect_metrics()
```

Visualize the classification tree.

```{r}
diabetes_tree_final_fit |>
  extract_workflow() |>
  extract_fit_engine() |>
  rpart.plot::rpart.plot(roundint = F, faclen = 2)
```

Produce a variable importance plot.

```{r}
diabetes_tree_final_model <- extract_fit_engine(diabetes_tree_final_fit) 
diabetes_tree_final_model |>
  vip(num_features = 15)
```

Generate a confusion matrix for final fit.

```{r}
tree_predictions <- 
  diabetes_tree_final_fit |>
  collect_predictions()

tree_conf_matrix <-conf_mat(tree_predictions, truth = Diabetes, estimate = .pred_class)
tree_conf_matrix
```

Collect and save final metrics for best Classification Tree model.

```{r}
tree_metrics <-
  diabetes_tree_final_fit |>
  collect_metrics()
tree_metrics
```

## Fit Random Forest model

**Random Forest** is an ensemble method that combines many decision trees using both bootstrapping and feature randomness. Bootstrapping involves creating multiple trees by randomly sampling the training data set with replacement. Feature randomness involves selecting a random subset of predictors at each split. Compared to a single tree or bagged tree, feature randomness helps mitigate the influence of a strong predictor or strong predictors that might otherwise create highly correlated trees. In classification, a majority vote from all trees determines the final prediction.

Set up Random Forest fit to use the `“ranger”` engine.

```{r}
rf_mod <- rand_forest(mtry = tune(),
                      min_n = 25,
                      trees = 500) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")
```

Create a workflow using the recipe and model defined earlier.

```{r}
diabetes_rf_wfl <- 
  workflow() |>
  add_recipe(diabetes_rec) |>
  add_model(rf_mod)
```

Create a tuning grid to consider varying levels of the number of predictors (`mtry`) that will be randomly sampled at each node split.

```{r}
plan(multisession, workers = 4)

set.seed(558)

rf_fits <-
  diabetes_rf_wfl |> 
  tune_grid(resamples = diabetes_5_fold, grid = 5,
            metrics = metric_set(mn_log_loss))

plan(sequential)
```

Sort models by tuned `mtry` parameter with lowest **log loss**.

```{r}
rf_fits |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

Identify the parameter specification with lowest **log loss** to inform the best model.

```{r}

rf_best_params <-
  rf_fits |>
  select_best(metric = "mn_log_loss")
rf_best_params
```

Using the best model, fit the model to the entire training data set using the `last_fit()` function. Compute the **log loss** and other performance metrics on the test set.

```{r}

diabetes_rf_final_fit <-
  diabetes_rf_wfl |>
  finalize_workflow(rf_best_params) |>
  last_fit(split = diabetes_split, metrics = metrics_set)

diabetes_rf_final_fit |>
  collect_metrics()
```

Produce a variable importance plot.

```{r}

diabetes_rf_final_model <- extract_fit_engine(diabetes_rf_final_fit) 
diabetes_rf_final_model |>
  vip(num_features = 15)
```

Generate a confusion matrix for final fit.

```{r}
rf_predictions <- 
  diabetes_tree_final_fit |>
  collect_predictions()

rf_conf_matrix <-conf_mat(rf_predictions, truth = Diabetes, estimate = .pred_class)
rf_conf_matrix
```

Collect and save final metrics for best Random Forest model.

```{r}

rf_metrics <-
  diabetes_rf_final_fit |>
  collect_metrics()
rf_metrics
```

## Select the best model

Compare all final models using both **log loss** and **ROC AUC**.

```{r}

rbind(tree_metrics, rf_metrics) |>
  filter(.metric == "mn_log_loss") |>
  mutate(model = c("Classification Tree", "Random Forest")) |>
  select(model, "mean_mn_log_loss" = .estimate)
```

```{r}

rbind(tree_metrics, rf_metrics) |>
  filter(.metric == "roc_auc") |>
  mutate(model = c("Classification Tree", "Random Forest")) |>
  select(model, "mean_roc_auc" = .estimate)
```

The Random Forest model performs slightly better on both metrics so that will serve as our final model.

Save the best model for use in the API.

```{r}
save(diabetes_rf_final_model, file = "model/diabetes_rf_final_model.RData")
save(diabetes_rf_final_fit, file = "model/diabetes_rf_final_fit.RData")
```
